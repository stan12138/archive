### 工具

写大的代码文件，一般使用spyder

但是在平时做一些简单的交互式测试的时候，一般使用jupyter notebook

在做网络编程的时候，常用的是sublime text3+cmd

#### jupyter notebook

打开一个命令行，输入jupyter notebook，即可运行起来notebook

最基本的，我需要设置字体，因为原本的字体实在太难看了，打开anaconda下面的Lib/site-packages/notebook/static/custom/custom.css，在里面添加这样的css`.CodeMirror pre {font-family: 'Source Code Pro';}`即可

呃，还有结果输出的样式：`div.output_area pre {font-family: 'Source Code Pro';}`

然后，运行的快捷键是shift-enter



修改起始目录：

在cmd中输入`jupyter notebook --generate-config`，将会生成一个名为`jupyter_notebook_config.py`的文件，找到`#c.NotebookApp.notebook_dir =`这一行，在等号后面填入想要的目录的完整路径，删掉前面的`#`，保存即可。

生成的文件应该在`C:\Users\Name\.jupyter`文件夹下吧，也许是在你的cmd当前目录下，修改后挪到前述文件夹即可。



### 有很多模块包的网站

[下载python模块的网站](http://www.lfd.uci.edu/~gohlke/pythonlibs/#pygame)

1，查看所有的模块列表，使用help('modules')

2、 urllib可以制作爬虫

3、如何存储文件到指定文件夹，关键在于获取路径，下面就是pythonista的文件路径:
'/private/var/mobile/Containers/Shared/AppGroup/1E6396A6-FB23-4A10-B1BB-B5F1F35BE42E/Pythonista3/Documents/picture/%s.png'%x

4、另一种方法，导入sys,使用sys.executable,可以获得pythonista3的路径

9,文件a已经导入了x模块，文件b导入了文件a，那么b就也间接导入了x模块

### 关于列表的拷贝技术：

列表是可变对象，直接传入，或者直接使用=进行拷贝，在改变时都会造成原列表的改变，因而，常用的列表拷贝技术是：a=b[:]
但是，这种方法并不安全，对于简单的一维列表，这种方式是有效的，但是对于结构复杂的列表这种方式在某些情况下往往会失效（并不是所有的情况下都会失效，甚至有一定的不确定性，我只隐约观察到了一点规律，并不确定），虽然使用id()去观察会看到两个变量是不同的物理地址，总而言之，对于复杂的列表，可靠的拷贝技术为：

~~~python
from copy import deepcopy

a = deepcopy(b)
~~~


这种方法是绝对安全的。此外，本方法用处很广，例如，类也是可变对象，传入函数是怎么办？怎么进行拷贝？deepcopy可以

==以后尽量少用import *==





## python包的经验


>这是一个关于python模块的重新说明，
>
>事实上这只是一个十分局限的说明，关于包
>
>2017/4/24 14:58:55 --stan

#### 为什么使用包？

大概也就是为了可以在常用的代码文件夹下创建一个已完成代码的文件夹，然后把不太会修改的代码放进去。

#### 要求与用法


- 首先，放置这些文件夹的上级或说更高级的目录必须是一个已经在搜索路径里面的目录
- 然后，记住python的import里面是使用`.`来代替`/`的
- 这条路径上的所有文件夹里面必须包含一个`__init__.py`文件，这个文件可以是一个空文件，当然这个文件也可以写一些代码，当导入发生时，这个文件里面的代码会首先默认执行，例如我们可以在这里面控制`import*`时会发生什么


#### 相对导入

例如文件夹的目录结构如下所示：

目录树画不出来了，直接描述吧

相对导入存在的意义就在于：解决导入路径的模糊性，例如当前文件所在目录有一个文件，恰好他的名字和默认路径上的某个文件名字一样，或者和某个内置模块的名字相同，那么，怎么确定导入的是哪一个呢？

关于语法，以当前文件所在路径为标准，一个点号回到该文件的父目录，两个点号回到父目录的父目录，使用方法如下： 

文件夹A下有两个文件夹：B和C，B下面有D和E，当前文件夹为E，则执行`from . import D`会导入文件D，执行命令`from .. import C`会导入文件C 

总而言之，相对导入存在的目的就在于让导入的路径更加精准，如果你可以确认不会因为路径不准确而发生任何错误，则不用考虑

#### 关于导出的内容的控制

假如有一个文件`imagetools.py`,我打算使用这个文件作为一个模块使用，但是我想防止当用户使用`from imagetools import *`时导出了太多的东西，我应该怎么办?
解决的方法就是在该文件中写一个列表，`__all__=[]`，在列表中使用字符串列出所有希望导出的变量的名字，那么当使用`import *`时将只会导出列表中列出来的变量

## 包与模块的重新说明

上面都是瞎扯。

这里我首先应该要阐述几个概念，模块，包，顶层文件，绝对导入，相对导入，搜索路径。

广泛意义上的模块指的就是一份py文件，我们可以引用其中的代码。包指的是数个模块或子包的集合，是一个文件夹，这个文件夹必须含有一个`__init__.py`文件，子包也必须包含一个这样的文件。

所谓顶层文件指的就是我们要运行的文件。顶层文件需要放在包之外。

导入最重要的步骤是搜索，系统会首先搜索出目标文件，搜索的顺序是当前目录，PYTHONPATH，标准库，.pth文件。

当前目录就是指同一个文件夹下，或者是子文件夹下的文件。

PYTHONPATH指的是环境变量

标准库就不说了，.pth文件指的是在上述路径上面的一个以.pth为后缀的文件所制定的路径

总而言之，导入就是这样的搜索方式。

对于顶层文件而言，永远都是上述次序。

那么什么又是绝对导入和相对导入呢？

这个时候，我们必须考虑严格意义上的包和模块，这些东西也只对狭义的包和模块起效。呃，准确而言，也许我把标准定的有点死了，但是最好还是按我的来做。

广义上的模块就是一个py文件，但是狭义上的模块必须是一个没有可以直接运行的代码的`if __name__=='__main__'`部分都不要有，狭义上的包只能包含子包和狭义模块（我的这个要求可能的确有点过分了，我想应该可以包含一些最初用来测试的非模块文件）。

那么对于包而言，出现了什么变化呢？python为包提供了一种特殊导入方式，称之相对导入，这里要记住相对导入只是对包有用的。

相对导入可以改变文件的搜索路径，例如你写了一个包，包中有两个模块，模块A和B，然后A想使用模块B的代码，很不巧这个时候B的名字和某个标准模块冲突了，例如叫做string，那么会发生什么情况呢？

似乎没什么，以为根据搜索路径先搜本目录，再搜标准库。但是事实上，这个搜索路径是针对顶层文件来说的，对于包中的模块而言，搜索次序并不是这样，他会把本目录的优先级放到最低。换句话说，你不可能拿到B模块。

那该怎么办，放心，这里便是相对导入该发挥作用了。针对包中的模块，python提供了相对导入的语法，这种语法对于顶层文件是无效的。相对导入可以以绝对的优先级改变搜索次序，具体而言，例如`import .string`指定了要导入的目标就是跟自己相同目录的string文件，再如`import ..t.string`指定了，我想要的是本文件的上层目录下的一个叫t的目录下面的string文件。总而言之，相对导入为包中的模块提供了一个在包里面任意目录下相对跳转的功能。

但是，我还是要重新说一遍，这个只对包中的模块有效，对于普通的顶层文件的文件夹没用。如果你使用了`..`调到了上一层，那么你必须保证上一层仍然在包内。

#### 总结

我不知道上面的我说清楚没有，总结一下。

对于普通的顶层文件，你想完成导入，那么只有绝对导入这个方法，你要么选择添加一个环境变量，然后在环境变量的辅助下完成文件夹的跳转；要么选择修改文件所在位置；要么添加一个.pth文件。别无他法。

但是当你想写一个包的时候，你可以使用相对导入，以帮助你在包内完成跳转。或者说，你先完成跳转，只能使用相对导入，因为你根本不知道用户会把你安装在哪。铭记相对导入语法只对包内模块有效。而包内模块绝对不能含有可直接执行代码。相对导入的跳转绝对不能跳出包的范围。



### 模块，包，路径

我觉得这一部分我必须继续，但是不是今天。我现在身体不舒服。

简单说一下先，上述只是在讲python文件，和import的情况，但是也存在着这样的情形，在一个包内，我们需要读去本包内的某个配置文件，或者是输出log到本包内。那么我们应该怎么写文件的路径？

此时`..`之类的东西是无效的，我们也不可能直接指定绝对路径，那么究竟是否有提供手段，让我们直接以相对路径的方式访问呢？答案是没有。

举例，文件夹tools是一个包，内部有a.py文件，该文件要读本包内的config.conf这个配置文件，包外有b.py调用了a，那么当运行的时候，当前程序的路径是按照b来确定的，也就是说a将会从b所在的文件夹找config，自然找不到，怎么让它知道要到包内找？方法只有一个，我们在a内写一行代码，让它确定出a文件所在的绝对路径，在根据此路径找到config，`os.path.dirname(__file__)`可以确定本行代码所在文件的绝对路径，`__file__`是内置变量



#### python代码组织形式

上面的的说明应该都是没问题的，换到实际操作中，大概可以这样总结，python中的模块导入，在不考虑系统的path的情况下，可以完成的目录跳转是本文件夹，子文件夹，上级文件夹，原则上这是完美的，但是问题是，上级文件夹的跳转只在包内有效，普通代码中无法使用。于是这就给代码组织带来了问题。

假设，有一个项目work，整个项目放置在一个文件夹中，所有这个项目的子项目都会使用a.py模块，每个子项目规范起见都应该新建一个文件夹，那么问题就是a.py应该放在哪里？明显应该在所有子项目的上级目录，但是子项目的代码文件是无法import上级目录的。

解决的方法是：

~~~python
import sys

sys.path.append("..")
~~~

原理是在路径中加入上级目录，并且这种修改是非持久的，非常好。





### 关于命令行的pip install

我使用

`pip install IPython --upgrade`
时出现了如下错误

`Cannot remove entries from nonexistent file d:\anaconda3\lib\site-packages\easy-install.pth`
此时需要使用如下命令：

`pip install IPython --upgrade --ignore-installed`

### 关于IPython

简直了，默认情况下，ipython中matplotlib的图像显示的方法是inline也就是嵌入式的，此时如果是动画就无法更新，所以我最喜欢的还是弹出式显示，但是我使用了网上给出的所有方法进行设置，我尝试过：
~~~python
	%matplotlib qt
	%gui qt
	%gui qt5
	switch_backend("Qt5Agg")
~~~
等等，各种方法，均无效，甚至导致无法显示图片
最后我终于解决了，直接在spyder的设置里面找到IPython的设置，然后设置graphic..里面的backend为自动，总之不要是inline就行，确认，重启spyder即可
不知道为啥以命令的形式设置就不行呢?

### functools

functools模块提供了一些用于函数的工具，例如偏函数

#### 偏函数

对于一个函数，也许有很多参数，或者说可以设置很多参数，对于一个特定的需求我们可能每次调用都要设置一次某个参数，很麻烦，但是偏函数可以帮我们把某个函数的某个参数设置为我们想要的默认参数，并返回设置后的一个新函数：
~~~python
	int('10',base=2)
	//这个可以把int函数设置为二进制转换，但是每次都需要为base设置为2，不然就是10进制的默认参数
	int2 = functools.partial(int, base=2)
	int2就是新的函数
~~~
若不加关键字，将会自动设置为最左边的参数

### 很牛叉的代码中运行代码

实现的效果是怎么样的呢？我可以写一个GUI或者Web页面，自制一个编辑器，接收收入的字符串，然后写入到一个.py文件中，然后使用代码运行这个文件，并接受输出结果，然后再放回到GUI中。 
~~~python
	import sys
	import subprocess
	
	a = subprocess.check_output([sys.executable,filepath],stderr=subprocess.STDOUT, timeout=5)
~~~
a就是filepath指的代码的运行输出结果，但是是一个二进制字符串，怎么解析就不必说了吧。  
稍微解释一下，check_output似乎是一个可以执行命令的函数，并给出返回值，第一个参数是一个列表，列表的第一个值sys.executable是一个字符串，它是本电脑上面python解释器的exe文件的路径，filepath是要执行的文件的目录，stderr是一个固定参数，用来处理错误的，换句话说如果运行出错会返回错误信息，timeout设定最长运行时间。

差不多就是这样了，再多的细节我也不太清楚。  

你如果很感兴趣的话，廖雪峰给了一个完整的网络代码编辑器的实现，见网址[python运行助手](http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001432523496782e0946b0f454549c0888d05959b99860f000)



### 关于目录和文件的操作

主要使用的模块是os和shutil

-   os.getcwd(),获取当前脚本的工作目录
-   os.listdir()，返回指定目录下的文件和目录名字
-   os.rename(old,new)，重命名
-   os.mkdir(dirname)，创建一个单级目录，可以指定完整路径名字，从而在其他磁盘创建
-   os.remove(filename)，删除文件
-   os.removedirs(dirname),删除目录,只能删除空目录
-   shutil.rmtree(dirname)，删除任意目录
-   shutil.copyfile(oldfilename,newfilename)
-   shutil.copytree(olddir,newdir)，将一个目录复制，newdir必须不存在
-   shutil.move(old,new)，移动


### 关于time

和time有关的有time和datetime两个模块，这里只简单的记录一两个函数

-   time.time(),时间戳，单位是秒，用来做计时还不错，无论windows还是Unix
-   time.sleep(),暂停，单位是秒，支持小数
-   time.clock()，在win上面第一次调用的返回值没啥用，但是之后每次调用都会返回与第一次调用的时间间隔，单位秒，还算可以，但是在Unix上面返回值一直都很诡异，还是别用了

### 关于signal

我现在了解的真的都很少，并没有细读任何东西，下面只讲一点

我们可以设置一个捕捉特定信号的处理函数，但是只能捕捉某些信号，

总而言之，它可以完成使用ctr-C结束多进程等的任务

~~~python
import signal,time

stop = False

def handler(a,b) :
    global stop
    stop = True
    print('get stop')
    
    
signal.signal(signal.SIGINT,handler) 

start = time.time()

while True :
    if not stop :
        if time.time()-start>2:
            print('2 seconds past....')
            start = time.time()
    else :
        print('I stop')
        break
~~~

就像上面所示，常规的使用方法是在信号处理函数中设置一个用于全局停止的变量，这里设置的signal.SIGINT可以捕捉ctr-C

使用方式就是使用signal.signal()设置信号的处理函数

需要注意的是，这个函数必须在主线程设置。



### 装饰器

这里只讲基本的装饰器函数。

~~~python
def t(func) :
	def t2(a) :
		print('this is stan')
		return func(a)
	return t2

@t
def test(a) :
	print('stan')
~~~

这里你要写一个函数，接受一个函数作为参数，然后当你使用@语法之后，实际上得到的是：

test = t(test)

接下来只需要分析t函数就可以了，t(test)的返回值是t2，所以test实际上变成了t2，当你调用test的时候就会先print('this is stan')，然后返回原本的test函数的返回值，实际上这里是没有返回值的，只是会运行一下test函数，但是，当有返回值的时候返回就有作用了

接下来要更进一步：

如果想让装饰器接收参数，如@t('stan')，那就必须为其再增加一层包装，因为t('stan')已经在执行一个函数了

~~~python
def t(b) :
	def t2(func) :
		def t3(a) :
			print('this is stan')
			return func(a)
		return t3
	return t2

@t('stan')
def test(a) :
	print('stan')
~~~

其实是蛮容易理解的吧

这里再给出一个flask里面用来设置url的装饰器，他的目的只是关联url和相应的处理函数，并把处理函数收集起来，所以它的装饰器原型是这样的

~~~python
	def set_url(self,url,method='GET') :
		def second(func) :
			if method=='GET' :
				self.get_handle_func[url] = func
			elif method=='POST' :
				self.post_handle_func[url] = func
			return func
		return second
~~~

使用的时候：

~~~python
@ap.set_url('/')
def index() :
	with open("static_file/homepage.html",'rb') as fi :
		rep = fi.read()
	#print('got html file')
	return rep
~~~

稍微分析一下，set_url是第一层包装，只是用来让装饰器可以接收参数的，然后才会返回一个基础版的装饰器即second，当使用这个装饰器的时候，就相当于在做，index = second(index)，于是我们就成功的收集到了index，这是只需要把他和url一起存入字典即可，至于return func，实际上并没有什么用，这是写一写

装饰器就讲这么多

### 生成器

这里讲的生成器指的是生成器函数，而不是表达式

基础的形式只有两种：yield value和value = yield

#### yield value

这种形式的生成器函数配合next()函数使用，它每次都运行到yield语句，然后当调用next()函数的时候，会返回value，然后继续运行，直至下一个yield

~~~python
def gen() :
	a = 1
	while True :
		yield a
		a = a+2

s = gen()
next(x)

~~~

每次调用next就会返回一个a的值，然后运行a=a+2，然后再次断在yield语句处

#### value = yield

这种形式的生成器函数配合send方法使用，同样每次到会运行到yield语句，然后等待从send里面获取一个值，然后继续运行，直至下一个yield

~~~python
def gen() :
	while True :
		a = yield
		print('I got ',a)

s = gen()
next(x)
s.send(1)

~~~

这里只是要注意，我们在调用send方法之前，必须确保生成器运行到了yield语句处，方法就是先调用一下next,这是规范化的一步，叫做预激协程

总而言之，首先使用next预激协程，使用send使协程工作，然后还可以调用close关闭协程，另外close也是可以捕捉的

当协程退出时，他会收到一个GeneratorExit异常，只要捕捉这个异常就好

#### 运行状态

事实上，一个使用生成器函数定义的协程有四个状态：GEN_CREATED,GEN_RUNNING,GEN_SUSPENDED,GEN_CLOSED

其中第一个状态是协程未启动状态，前面的value = yield，当只是执行了s=gen()之后，就出在这个状态，这个时候如果你调用send就会收到一条错误消息，消息很清楚地表示了生成器的状态，这是必须执行预激活

当解释器处在运行中，就是第二种状态

在yield处暂停是第三种状态



#### 复合语句

我们可以把`value = yield`和`yield value`两种语句结合在一起

~~~python
def gen3() :
	a = 2
	while True :
		b = yield a
		print('I got ',b)
s = gen3() #1
next(s)    #2
s.send(3)  #3

~~~

这里，我暂时无法做一些解释，我只能描述他工作的方式

注意，首先我们从前面可以观察到yield value也是会暂停的，当使用next时才会继续运行

然后a = yield也会暂停，需要执行一次send

所以，可以按照这个分析一下上面的代码怎么运行：

1、第一步必须执行next(s)，预激活，然后程序会执行到yield a，返回a，然后等待

2、第二步应该执行send,程序继续运行，到yield a之前

接下来每执行一次next,返回一个值，再执行一次send接收一个值

当然可以连续执行两次next，那么就会收到一个None

天衣无缝是不是？但是，事实并非如此

事实上的常规运行是：预激活，然后不停调用send就行了，程序只会在b=yield处产生断点，yield a则会自动执行，并不需要调用next，如果调用了next则相当于send(None)

~~~python
def gen3() :
	a = 2
	while True :
		b = yield a
		print('I got ',b)
        
        
s1 = gen3()
next(s1)
out :2
s1.send(5)
out: I got 5
out: 2
next(s1)
out: I got None
out: 2
~~~

如上所示

总而言之，断点只有b = yield，而yield a会自动执行，除了预激活之外不需要使用next

我知道很诡异

#### 预激活装饰器

如果忘记预激活了，就很麻烦，所以可以使用一个装饰器，自动完成预激活

~~~python
from functools import wraps

def coroutine(func) :
	@wraps(func)
	def primer(*args,**kwargs) :
		gen = func()
		next(gen)
		return gen
	return primer

@coroutine
def test() :
	total = 0.0
	count = 0
	av = None
	while True :
		term = yield av
		total += term
		count += 1
		av = total/count

b = test()
b.send(2)
~~~

用法完全一致，只不过可以少了预激活

前面的wrap装饰器可以直接忽略掉，因为他只是用来弥补装饰器的缺陷的，因为是用完了装饰器之后func的`__name__`等属性会改变，wrap只是把它改回来

#### 终止协程与异常处理

截止到目前为止，如果向协程send了不合乎要求的值，将会直接导致抛出异常，协程也将终止，并且无法使用send再次激活

这实际上说明了可以使用这种方式终止协程

生成器提供了两个方法，throw和close

后者可以直接关闭生成器，前者可以向生成器传入一个异常，如果想在关闭时做一些操作，则需要使用try...finally

~~~python
def test() :
	total = 0.0
	count = 0
	av = None
	try :
		while True :
			term = yield av
			total += term
			count += 1
			av = total/count
	finally :
		print('stop')
  
~~~

#### 生成器的返回值

如果想要通过return拿到生成器的值比较麻烦，因为生成器的停止一定会抛出异常，返回值将以异常的value的形式给出，所以，我们必须这样：

~~~python
def gen3() :
    a = 2
    while True :
        b = yield a
        if b==1 :
            break
        print('got ',b)
    return 10
s = gen3()
next(s)
try :
    s.send(1)
except StopIteration as t :
    ret = t.value
~~~

ret就是返回值



#### yield from

这是一个新增的语法，我认为它存在的目的就是为了解决如何在一个生成器中调用另一个生成器，如果你想要尝试一下手工实现这个目的，并完整的实现输出，传递数据，处理异常，各种方法调用，你就会知道，这也是一个比较麻烦的问题，yield from解决了这所有的问题，如此。

假设函数A是：`yield from B()`

B()会返回一个可迭代对象（或者生成器），A也会返回一个生成器，分别称之b,a

那么，借用一个博客上面的记录：

>   1.  b迭代产生的每个值都直接传递给a的调用者。
>   2.  所有通过`send`方法发送到a的值都被直接传递给b. 如果发送的 值是`None`，则调用b的`__next__()`方法，否则调用b的`send` 方法。如果对b的方法调用产生`StopIteration`异常，a会继续 执行`yield from`后面的语句，而其他异常则会传播到a中，导 致a在执行`yield from`的时候抛出异常。
>   3.  如果有除`GeneratorExit`以外的异常被throw到a中的话，该异常 会被直接throw到b中。如果b的`throw`方法抛出`StopIteration`， a会继续执行；其他异常则会导致a也抛出异常。
>   4.  如果一个`GeneratorExit`异常被throw到a中，或者a的`close` 方法被调用了，并且b也有`close`方法的话，b的`close`方法也 会被调用。如果b的这个方法抛出了异常，则会导致a也抛出异常。 反之，如果b成功close掉了，a也会抛出异常，但是是特定的 `GeneratorExit`异常。
>   5.  a中`yield from`表达式的求值结果是b迭代结束时抛出的 `StopIteration`异常的第一个参数。
>   6.  b中的`return <expr>`语句实际上会抛出`StopIteration(<expr>)` 异常，所以b中return的值会成为a中`yield from`表达式的返回值。

从外在表现上面看，你会感觉到，你对a的操作完全就是在直接操作b，没有任何区别，因此一系列表现与上面讲的生成器没有区别。



#### 一些关于输出的小把戏

我主要感兴趣的就是原地显示和颜色控制

##### 原地显示

所谓原地显示指的是覆盖上一个字符，始终在原处显示，主要为了表现一种等待的状态，这里主要使用的是转义字符，据我所知控制输出位置的字符有两个`\r`和`\b`，前者的表现是将输出位置转移到本行的最前端，然后当你继续输出的时候就会依次覆盖前面的字符，后者的表现是输出位置前移1个位置，也就是说会覆盖前一个

借助`\b`，我们可以实现一个原地旋转的`\`，当然要配合一下time.sleep，关于sys.stdout.write和print的区别我就不必多说了吧，总之前者是无格式输出

这里给一个样例：

~~~python
import time
import sys

def test() :
    a = ['\b-','\b\\','\b|','\b/','\b-','\b\\','\b|']
    print('this is test e',end='')
    for i in range(5) :
        for j in a :
            sys.stdout.write(j)
            time.sleep(0.4)

test()
~~~

这个，在普通输出的时候的确工作的蛮好的，那么在windows命令行下的表现如何呢？你以为就这样就ok，那你想的就太简单了，在命令行下如果你运行这样的代码，你会发现输出会卡死。

怎么解决？很可悲，百度似乎并找不到结果，google了之后找到了一个答案，答案是输出是存在缓存的，所以结果并不会立即展示，所以我们需要手动刷新，方法很简单只需要在每一个time.sleep下面加一行sys.stdout.flush()就好

##### 颜色控制

颜色控制来自系统，与语言无关，他是一串转义序列，我们可以将其嵌入到字符串中，下面给出这个特殊转义序列的书写格式：

`\033[显示方式;前景色;背景色m`

我们只需要将这串序列嵌入到字符串中，就可以使接下来的输出全部以指定的颜色显示，直至我们做了新的更改。其中的显示方式是使用数字标识的：

-   0,终端默认设置
-   1，高亮
-   4，下划线
-   5，闪烁
-   7，反显
-   8，不可见
-   22，非粗体
-   24，非下划线
-   25，非闪烁
-   27，非反显

前景色和背景色也是用数字表示，前景色是30-37，背景色是40-47，对应的数字颜色是一致的

-   x0，黑
-   x1，红
-   x2，绿
-   x3，黄
-   x4，蓝
-   x5，紫红
-   x6，青蓝
-   x7，白

系统默认的样式是`\033[0m`

举一个栗子：

~~~python
print('\033[1;31m')
print('this is stan test')
print('\033[0m')

print('\033[1;31mthis is stan test\033[0m')
~~~

但是你以为这么就结束了吗？naive。

这种代码是无法直接使用在logging的format里面的，至少在使用配置文件和命令行的情况下是不行的。

即便只是单单的命令行，也是无法使用的。

换句话说，在windows命令行中这种代码是无法使用的。想要在命令行使用，我们必须调用win的设置，使用ctypes：

~~~python
import ctypes

std_out_handle = ctypes.windll.kernel32.GetStdHandle(-11)

def set_color(color,handle=std_out_handle) :
	ctypes.windll.kernel32.SetConsoleTextAttribute(handle,color)
def reset() :
	set_color(0xf0)

black = 0xf0
red = 0xfc
yellow = 0xfe
~~~

这里面的获取handle的部分，-11是标准输出的代码，-10是输入，-12是标准错误

然后在颜色设置的时候，需要使用两位十六进制代码，其中的高位代表背景色设置，低位代表前景色，对应的色彩是一致的：

0-f分别为：

| 代码   | 颜色   |
| ---- | ---- |
| 0    | 黑    |
| 1    | 暗蓝   |
| 2    | 暗绿   |
| 3    | 暗天空蓝 |
| 4    | 暗红   |
| 5    | 暗粉   |
| 6    | 暗黄   |
| 7    | 暗白   |
| 8    | 暗灰   |
| 9    | 蓝    |
| a    | 绿    |
| b    | 天空蓝  |
| c    | 红    |
| d    | 粉    |
| e    | 黄    |
| f    | 白    |

我们每个输出最好先设置色彩，然后reset









### 关于raw字符串

其实raw有的时候还是挺方便的，尤其是在matplotlib里面写latex的时候，我想你大概也并不愿意每次都自己手动将`\`变成`\\`吧

但是，这个时候考虑一个问题，我们是否可以将普通字符串变成raw字符串？有没有什么函数，或者replace的方法，事实证明，你如果一开始就已经把变量声明过了，那么replace是无效的，因为转义字符在初始化的时候已经生效了，并且没有任何函数具有这种转换的功能，换句话说如果已经写了`a='adsd\ndwe\tasxwe'`这样的语句，我们基本上没有办法再把a转换为raw了，至少我搜索了这么久没找到可行的方法。

事实上，在普通用途下，这个似乎也没有什么必要，如果你需要一个raw，那么为什么不在一开始就把它变成raw呢？这完全是可控的。

但是，如果获得的字符串是用户输入的该怎么办？强制要求用户将每个`\`变成`\\`不太人性化，但是我们有没有办法在获取之后再转换，是不是很尴尬。

但是，如果你试验一下，你会发现input竟然很人性化的，自动将输入的`\`变成`\\`，牛叉。虽然当你的确想要转义的时候这显得有点恶心，但是大多数情况下并不是。



### 邮件发送

很多情况下，使用程序自动发送邮件还是很有用的，在python里面使用的工具主要是smtplib和email两个模块。

首先要解决的第一个问题是客户端的问题，我们需要一个邮件发送的服务器，真正靠谱的方式还是要使用第三方的服务器，例如我们注册了qq邮箱，就可以通过一些设置和授权，允许我们的程序使用qq邮箱服务器发送邮件。

对于qq而言，邮件发送服务器的地址是`smtp.qq.com`，想要登陆需要授权码，首先打开网页版qq邮箱，然后找到s设置-账户-POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务，开启POP3/SMTP服务，这个过程需要你的邮箱有绑定手机或其他的安全设置，还会要求你发送短信到指定号码，然后就会生成一个授权码，授权码可以一直使用，也可以多次生成。

下面给出一个简单的例子：

~~~python
def send_email(ip) :
	user = "xxxxxxx"
	code = "xxxxxxx"

	reciver = "xxxxxxxx"

	msg = MIMEText("My IP is "+ip)
	msg["Subject"] = "this is my ip"
	msg["From"] = user
	msg["To"] = reciver

	try :
		smtp = smtplib.SMTP_SSL("smtp.qq.com")
		smtp.login(user, code)
		smtp.sendmail(user, reciver, msg.as_string())
		smtp.quit()
	except Exception :
		print("send fail...")	
~~~

这里的user就是qq邮箱账号，code是授权码，reciver是收信人，注意code对于现在的qq邮箱来说应该是16位的，生成的时候你可能会以为有空格，但是应该去掉空格。

当然还有更高级的功能可以使用。



### 关于线程与阻塞

我遇到了一个让我十分不爽的问题。

我在做一个低级版本的文件传输工具，也就是纯命令行的，然后，按照计划，两个客户端这件事可以双向选择的，他们都在等待用户输入通信的伙伴，然后，如果其中一方已经主动选择了，那么对方应该退出选择界面。于是这里产生了问题，选择是运行在一个线程中的，并且是一个input操作，毫无疑问，我们不能或者绝对不应该强制关闭掉一个线程，所以说线程的退出应该是主动的，建议通过轮询实现，可是如果万一线程中包含了阻塞式操作，那就尴尬了，这时我们要求这个阻塞操作应该是可以设置timeout的，否则就彻底完蛋了，但是，尴尬的是input没有超时。虽然可以通过signal等工具实现类似的超时，但是signal的处理函数只能用在主线程中。靠！

欸，稍等，也许我可以将input放入主线程。。。。





### Timeout

找到一种技术，可以给任意函数加装一个timeout的装饰器

现在我知道的可以供使用的技术有两种，分别是借助singnal模块的alarm，另一个就是借助threading的Timer。

其中，前者只在Unix平台可用，并且只能在主线程中可用

~~~python
import signal
import time

class Stop(Exception) :
	pass

def work() :
	try :
		for i in range(5) :
			print(i)
			time.sleep(10)
	except Exception as er :
		print("stop", er)


def work_done(signal_num, frame) :
	raise Stop


signal.signal(signal.SIGALRM, work_done)
signal.alarm(20)
work()
~~~

这是一个实例，系统会自动计时，当设置的时间到时，就会向主线程调用handler函数，同时传入两个参数，分别是信号数目和当前堆栈frame。。。我也不清楚这是啥。

然后技巧上，利用这个信号自动终止在运行的函数的技巧就是异常，这是一个很通用的技巧，当我们想终止正在运行的函数的时，异常是一个合理的方法。

只需要将这个技巧包装成装饰器就可以了，但是，还是那句话，限制条件要记清楚，Unix和主线程



呃，刚才研究了一下timer，发现似乎很是鸡肋

~~~python
import threading
import time

def work() :
    for i in range(5) :
        print(i)
        time.sleep(5)
        
def handler() :
    print("time up")
    
ti = threading.Timer(10, handler)
ti.start()
work()
~~~

timer不阻塞，当时间到的时候，会自动调用handler，似乎一切都很完美，但是，可以看一下timer的实现方式。

大概上，他的逻辑是首先实现一个子线程，然后让这个线程sleep设置的timer的时间，然后调用传入的handler。

所以，就可以看出来，raise方法将无法使用，因为timer自己处在一个子线程中，其他线程将无法再接收这个线程发送的Exception



当然也有一些其它的，变形的使用threading实现的为普通函数提供timeout的方法，但是它们的一个普遍的缺点是将开启一个关不掉的线程，直至程序结束，这就造成了资源的浪费。

详细的我没有看，总而言之，结论就是，在python里面，没有一个全平台的，特别通用，特别完美的为普通函数实现timeout的方法

网上的一些实现使用了多线程的join方法来实现普通函数的timeout装饰器，但是由于无法杀死线程，所以当函数运行超时的时候，实际上他还是在后台运行，所以这种方法不太好。

为了解决这个问题，网上其它人给出了使用多进程的版本，因为进程是可以杀死的，但是多进程也是有不少限制的，所以。

并且，似乎对于线程，我们也无法设置timeout，并且没有好的方法实现。





### Python编码

向来都说python编码问题让人很头疼，但事实上，这一点让我很困惑，编码问题确实会让人很头疼，可是我觉得这个问题并不是单独局限于python的，我觉得这应该是一个普遍的问题。



#### 编码

数据的存储明显都是二进制，如何解释这些二进制内容，就是编码。接下来主要说一下ASCII,Unicode，UTF-8，以及中文编码。

早期的编码方案是ASCII，它使用一个字节做为单位，但是事实上只应用了7位，也就是说只定义了其中的128个的意义。

之后的扩展ASCII，又给剩余部分定义了很多其他的特殊字符。

当计算机推广到非英语国家的时候，这些国家自然也面临着编码问题，于是不同的地区采用的不同的编码方案，他们都是在ASCII的基础上做了自己的扩展。于是分歧就产生了，这就导致了不同编码方案的不兼容。

特别的对于中文而言，ASCII的单个字节的编码方案只剩余了128个码，肯定是远远不够的，所以中文编码一开始就使用了双字节编码，早期的中文编码方案是GB2312，这种方案规定使用两个字节编码一个中文字符，其中的每一个字节都要兼容ASCII，也即高低两个字节的值都是大于127的。在这种编码方案里面，除了编码了一些常用汉字之外，还把一些ASCII中已经编码过的符号也重新使用双字节定义了一遍，这种情况下的符号称为全角符号，对应的ASCII里面的符号称为半角符号。

再后来，人们想要更多的汉字，于是就对GB2312方案做了修正，此时不再要求高低两个字节都要大于127了，只要其中的高字节大于127即可(大概是这样吧，可能不是很准确)，这种汉字编码方案称之GBK。

实际上GBK的编码范围是很广泛的，编码了足够多的汉字之后，依旧有很多剩余。再后，又在GBK的基础上，将一些少数民族的字符编码进去，形成了GB18030编码。

所以，基本上中文编码就是这样的，中文常用的编码就是GB2312,GBK,GB18030。

但是到此为止，已经形成了很大的分歧，各个地区和国家所应用的编码标准都是不一样的，例如同样是中文编码，大陆和台湾的方案似乎就不一样。这种问题造成的结果就是没有统一的标准，文档不能通用，不能以一个一致的标准进行解释。

所以，我们应该有一个一致的编码方案，于是Unicode编码诞生了，它是由一个国际组织制订的，并在不停的发展中，目标是囊括全世界所有的字符和符号。具体的Unicode是怎么编码的，细节我不是很清楚，感觉似乎它似乎也是一个变长编码方案，说不准，但是这些不重要。有一点需要说明的是，Unicode似乎最小也使用了两字节进行编码。

但是Unicode只是一个编码方案，或者说是一个标准，并不是真正的实现方案。因为它的目标是囊括所有符号，所以，必然造成了资源的浪费。例如对于英文符号来说，一字节已经足够编码了，他依然使用了两个字节，并且高位都是0，这就造成了空间的浪费，资源的浪费。所以，实际上Unicode早期的使用者并不多。直至互联网的广泛发展，带来了一些Unicode的实现，这些方案同样使用了Unicode标准，但是采取了一些措施去除了Unicode中的无效空间的浪费。

其中UTF-8就是一种代表。UTF-8全称8位Unicode传输格式，这意味着UTF-8使用的就是Unicode的标准，但是它是一种传输格式，每次传输一个字节的内容。直到后来，UTF-8不再仅仅应用于传输，而是被普遍使用了。

UTF-8是一种变长编码，它采用了1到4个字节进行编码，例如ASCII编码可以表示的，就应用一个字节表示，此时最高位是0，当需要两个字节表示的时候，高位字节的最高两位就会是11，同时第三位是0，代表这个编码采用了两个字节。诸如此类。

具体的细节不需要再了解太多，基本上这样就足够了。

#### 内容编码

对于python来说，我认为最常见的，也是相对比较简单的就是内容编码。他的意思是文件内容的编码。

例如，在写爬虫的时候，我们拿到一份网页的二进制内容，如何对他进行解码，这就是一种内容编解码问题。再如，当我们在读写文件的时候，例如txt文件，对其内容又是如何进行编解码的，这也是一种内容编解码问题。

这个问题的简单之处在于我们可以自由掌控内容。

例如，我们使用requests获取了一个网页，使用其content属性拿到了这个网页的纯二进制内容，将其存入`con`变量，那么就可以使用`con.decode(encode-name)`进行解码，例如`con.decode("gbk")`。此时，以gbk和utf8为例，两种编码明显都有一些特殊的规定，例如某一个字节的最高位应该是什么样的，此时如果采用了错误的解码方案，那么很可能会直接引发错误，这是就可以知道采用了错误的解码方法，然后修正。但是也有可能很巧，刚刚好采用了错误的解码方案，但是标准都符合了，于是就会得到一个不知所谓的奇奇怪怪的乱码。这种情况下，相对要棘手一点，以为自动化的程序无法发现这个问题。一种解决方式是使用一些用来探测编码的模块，例如`chardet`

虽然有这些问题，但是都不碍事，因为解码错误的会报错，我们可以修改，没报错的虽然是乱码，但是它也意味着原始的二进制内容并未有任何损失，后期发现之后可以重新以二进制格式读入文件，然后重新解码即可(前提是decode方法中的errors参数不要指定为ignore等值，因为这样它们会以不同的方式修改原始二进制内容，或丢弃，或替换等)

举个例子：

~~~python
#以指定编码，写入内容，为了达到目的，必须使用二进制
content = "哈哈哈哈，who are you"
with open("stan.txt", "wb") as fi :
    fi.write(content.encode("gbk"))
    
    
#读出内容
with open("stan.txt","rb") as fi :
    content = fi.read()
    
print(content.decode("gbk"))
try :
    print(content.decode("utf-8"))
except Exception as er :
    print("decode error",er)
~~~

虽然在python的编码体系里面，默认的编码是utf-8，但是通过原始二进制内容的处理，我们依旧可以自由的控制内容的编码格式。

例如普通一个字符串`即('str')`，在默认编码体系下肯定是utf-8编码的，这意味着他在内容中对应着一个采用了utf-8格式的二进制串，但是我们可以通过这个字符串的`encode`方法，将这个内容编码成我们想要的任意编码的二进制串，只是我们不能得到展示为我们指定编码的普通字符串而已，因为在python默认编码体系下字符串都是utf-8的，但是我们可以将这个特定编码的二进制串写入文件，正如上面的示例所示。

总而言之，这一切意味着对于内容编码，我们是相当自由的，并且可以任意做控制，理论上这里来说虽然有时候也会被编码问题烦，但是我们因为有自由控制权，所以都不叫事。

这里再说一个测试方法，例如某些情况下我们知道发生了乱码问题，但是又不知道发生错误的地方到底采用了什么编码，那么应该怎么办呢？例如，有一份文件，我们以二进制形式写入了utf-8编码下的一些内容，然后在某些阅读软件打开了这份文件，但是显示出来的是乱码，我们想知道，这个阅读软件采用了什么编码去读取，但是这个软件又不提供修改和显示编码的功能，该怎么办。

~~~python
s = "我的老大"
with open("stan.txt",'wb') as fi :
    fi.write(s.encode("utf-8"))
    
    
#阅读软件显示的内容是：鎴戠殑鑰佸ぇ
s1 = s.encode("utf-8")
print(s.decode("utf-8"))  #输出：我的老大
print(s.decode("gbk"))    #输出：鎴戠殑鑰佸ぇ
~~~

这样我们就可以知道这个阅读软件使用了gbk的汉字编码，那么我们写入的时候只要encode成gbk即可。

明显，这种对比方法我们是需要一定的猜测范围的，一般对于常见的中文来说，可能的编码就是GB2312，GBK，GB18030。

总之，要注意的就是，在默认的编码体系下，python中字符串就一定是已经经过utf-8进行编码的，想要获得不同的编码一定要使用二进制串，这是唯一的方法。仔细想一下，这个是很容易理解的。

总之，内容编码不是太大的问题。



#### 文件名编码

真正让我头疼的是文件名的编码，它相比内容编码的不同之处在于，我们似乎没有完整的控制权，并且这一次是要和操作系统打交道，而系统是不是偷偷干了什么对我们来说基本上就是黑箱。

一切问题的核心在于，我们几乎没有办法创建一个文件，将文件名指定为想要的编码格式，最直接的想法是这样的：

~~~python
name = "哈哈.txt"
with open(name.encode("gbk"), "w") as fi :
    fi.write("hello")
    
~~~

还是那句话，默认情况下，想要在python中使用特定编码，只能使用二进制串，所以我们只能使用二进制串指定特定编码的文件名。

但是，事实上，直接运行上述代码得到的结果是报错，应该是说utf8无法编码某个位置这样的错误

这是什么原因？



话分两头，为什么会关注这个问题？我碰到了这样的情况，在windows平台上，使用python创建文件，然后存储到iOS某些软件中，结果发现有些出现了文件名乱码，有些出现了内容乱码。针对内容的乱码，解决方案也比较简单，因为如上所述，我们可以自由控制内容的编码，一般而言，只要采用UTF-8就不会有问题。但是文件名的乱码就让人很头疼了。而解决文件名乱码的最直接想法就是，怎么实现自由控制文件名编码？

我花了不少时间，尝试和查阅资料，结论基本上是：我没有办法，也许可以做到，但是我没找到怎么做。但是实际上这并不妨碍大多数情况下解决上述问题。



首先说明一下上述示例出问题的原因。

这里借用一下别人的总结，他们认为单独对于python自身来说，存在着三个层次的编码，分别是脚本字符编码，脚本存储编码，解释器编码。

解释一下，所谓脚本字符编码就是指程序员指定或者声明当前代码文件采用了什么编码，具体来说它指的就是我们偶尔可以在代码文件一开头看到的：`# coding:utf-8`类似的字样，这实际上是一个类似于C的预处理指令一样的东西，所以，虽然它看起来只是注释而已，但事实上是一段功能性的代码。具体的这个是在PEP263中提出的，它指出用户可以在代码文件的前两行中任意一行以类似的形式指出解释器应该以何种编码去解释代码文件，系统会通过正则表达式进行释读。

脚本存储编码其实类似于之前说的内容编码，因为代码文件本质上也是一个文件，而其中的代码也就是文件内容，所以它也是以一定的编码存储在存储设备中的。这个存储格式是由编辑器决定的。例如对于提供了这种功能的编辑器，我们可以选择以何种编码保存。

明显脚本字符编码和脚本存储编码是相对应的，而这应该保持一致，脚本存储编码决定了代码以何种编码存在磁盘中，脚本字符编码告诉解释器从磁盘中取出来的二进制格式的代码应该以何种编码释读，如果二者不一致一般都会出问题的。

在python3里面脚本字符编码一般不需要特殊声明，默认就是utf-8，而绝大多数编辑器应该也是会默认采用utf-8编码。所以，我们是不需要自行指定的，绝大多数情况下也是毫无意义的。当我们没有权限修改编辑器的存储编码，或者不知道编辑器的存储编码的时候，擅自去修改脚本字符编码更是十分愚蠢的。

总而言之，这两个东西实际上没有什么太大的用处。

额外提一下，当我们想实现自定义代码存储编码，但是编辑器又不提供这样的功能的时候，我们应该怎么做？事实上，可以通过一个辅助的代码来实现，可以将代码写成字符串，编码成要想的编码格式，然后通过python代码以二进制格式写入`.py`文件即可。

最后是解释器编码，其实这个更没的说，解释器编码指的就是解释器以何种编码解释代码文件，默认python3就是utf-8，当指定了脚本字符编码的时候，也许解释器就会换用指定的编码解释，也许是执行了转换到utf-8再解释，但是这些都无所谓了，因为这完全是透明的，无权干涉，也没有必要去干涉。

于是，可以解释上面的示例出问题的原因了，简而言之就是在默认情况下，python认为代码文件是utf-8编码，但是却给open函数传入了一个二进制序列，open函数会试图用utf-8去解码这段二进制，但事实上这段二进制是gbk编码，于是就出错了。

网上有人给出了这样的代码：

~~~python
#coding:gbk

name = "我的老大.txt"

with open(name.encode("gbk"), "wb") as fi :
    fi.write(b"hello")
~~~

他们认为这样就可以实现创建一个文件名是gbk的编码的文件，但事实上呢？并不行，虽然这个代码可以正常运行。

首先代码能够正常运行是因为，虽然指定了脚本字符编码为gbk，而脚本存储代码是utf-8，但是恰好脚本里面的汉字使用错误的编码解释只会发生乱码，而没有发生异常，所以代码文件可以正常执行。

然后就是为什么不行，这是由接口决定的，open函数的特性是，一般只接受字符串，但是也可以接受二进制，但是执行的时候实际上还是将二进制串解码成了字符串，解码过程使用的编码要依据情况而定，默认utf-8，这就是最前面的示例失败的原因，因为utf-8无法理解gbk二进制串。后面这个成功了，是因为指定了脚本字符编码是gbk，那么解释器就会用gbk去理解这个字符串，自然可以。

所以，这样可以看出一切的关键在于我们可否突破open的限制，直接就是用二进制串，而不经过黑箱式的转换。

结论是，似乎不行，因为这是一个和系统打交道的过程，事实上Windows系统的文件系统的编码很复杂，甚至并不是我们想象的utf-8，而是utf-16，python在背后帮助我们完成了一切的转换，同时也将这些全部透明化了，方便了，但是也的确丧失了控制权。这意味着我们实际上根本得不到windows文件系统下文件名的原始二进制串，一切都是转换过的。这些在PEP529中有，可以去看看，我其实没太看懂。

这实际上还牵扯着另外一件事，这么长我们都在讨论文件名的编码，但是我们实际上根本没有一个金标准，我们根本没有办法知道文件名的真正编码，也无法确定我们真的得到一个gbk编码的文件名，根本没有办法检测，所以前面做的所有其实有点扯，就好像根本没有可以作为对照的绝对标准，就在这边瞎讨论对与错，毫无意义。

实际上好像有一些软件可以完成这件事，它可以实现真正的检测和修改文件名字的编码，但是我没有尝试。

网上有一些人给出了python下的这种手段：`names = os.listdir(b".")`。理由是如果给listdir函数传入二进制内容，返回值也是二进制的，的确如此，返回值的确是二进制，但是还是自动转换过的，依旧是utf-8，也是没啥用。那么传入一个gbk编码的二进制会怎样？这就回到了脚本字符编码的问题，没有指定脚本字符编码，擅自传入其他编码二进制，只会报错而已。



所以，一言蔽之，很无奈，陷入了死境。



那么前面提到的问题解决是怎么回事，这里要说回问题的详细表现了，实际上我从windows转存到ios的并不是单一的文件，而是大量文件，然后打包成了zip。打包方式有两种，一种是360压缩这样的压缩工具打包，一种是利用python的zipfile打包。经过检查可以发现，两者的打包结果并不一致，而文件名的乱码实际上是出在这里。尝试发现对于很多外国产的软件python打包结果可以正确解码，得到正常文件名，而360的就是乱码；而国产的一些阅读软件情况恰好相反。

从道理上来分析，我感觉python的打包可能要更符合标准，而问题的出现可能应该说是：水土不服。。。。

所以解决方法就是尝试发现不同软件适合的打包方式，然后传输即可。



也就是说其实以上文件名编码是毫无意义的，只要你不瞎胡搞，系统之间应该会完美配合完成一切的透明化操作。如果出问题了，大多是用的一些奇奇怪怪的软件在搞鬼，例如压缩工具。

但是，至少我了解了一下深层次一点的编码。



### 迭代器、生成器与协程

这些东西，其实我一直都没有太搞懂过，直到看了别人的讲解视频。

#### 迭代器

所谓迭代，就是可以逐个取出元素，直至访问完了所有的元素。所谓迭代器可以说是可以被next函数逐个访问下个元素的对象。

简单来说，实现了`__iter__`这个魔法方法的类就是一个可迭代的类，可以使用`isinstance`来和collection里面的`Iterable`类来比较，如果某类是`Iterable`的子类，那么它就是可迭代的。

但是可迭代的类的对象并不代表就是迭代器，如果要成为迭代器，`__iter__`方法必须返回一个对象，这个对象是可迭代的，同时实现了`__next__`这个魔术方法。然后for语句对于迭代器拿到的逐个元素的值就是`__next__`方法返回的值。

Ps. 我试了一下，如果返回的对象不是可迭代的，而只实现了`__next__`方法，似乎也是可以的。

~~~python
class Fib :
    def __init__(self, n) :
        self.num = n
        
    def __iter__(self) :
        return F(10)
    
class F :
    def __init__(self, n) :
        self.num = n
        self.now = 0
    def __next__(self) :
        if self.now<self.num :
            self.now += 1
            return self.now
        else :
            raise StopIteration
            
for i in Fib(1) :
    print(i)
~~~

这里要注意，isinstance应该这样使用`isinstance(Fib(1), collections.Iterable)`，然后for会自动在收到`StopIteration`这个异常的时候终止，所以，有了以上知识，实际上就可以实现一个斐波那契数列的迭代器了。

自然，在真正写一个迭代器类的时候，我们不会傻傻的建两个类，而是建一个类，同时实现上述两个魔术方法实现在同一个类中，然后`__iter__`中返回self

##### 迭代器有啥用

迭代器的存在在于节省内存空间，python2中，存在range和xrange两个函数，前者直接返回一个列表，后者返回一个迭代器，可以看到很多地方都在使用xrange，因为迭代器在于只保存生成下一个值的方法，而列表需要一下保存整个列表，所以，如果需要迭代100亿次，先生成一个100亿的列表自然很傻，而生成一个100亿的迭代器自然就绝对明智了。这就是迭代器的好处所在。

在python3里面，取消了xrange而把range改成了迭代器，当我们用list(range(10))的时候就可以拿到列表，否则就是一个迭代器，而list不是类型转换，而是类似于for语句，调用next从迭代器逐个取出元素，然后放入一个列表。



#### 生成器

一言蔽之，生成器就是特殊类型的迭代器。制造一个生成器有两种方式，第一是将列表迭代的`[]`换成`()`，如：

~~~python
[x**2 for x in range(10)]

#[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]

(x**2 for x in range(10))
#<generator object <genexpr> at 0x00000058508EB308>
~~~

上例生成器的好处还是在于节省空间，他保存的还是生成方法，而不是结果。

第二种生成器的创建方式就是yield，一旦函数中出现yield，它就不再是函数，而成为了生成器，生成器的特点在于可以暂停执行，保存断点，然后继续执行。

~~~python
def test(num) :
    n = 0
    while n<num :
        yield n
        n += 1
for i in test(10) :
    print(i)
~~~

生成器是特殊迭代器，所以自然也可以使用for

上例中`a = test(10)`的a是一个生成器，调用`next(a)`可以驱动a执行，它会执行到yield，将yield右侧的值返回，并暂停，下一次再调用next，会从断点开始，再次执行到yield。

直至最后，会自动`raise StopIteration`，如果给迭代器指定了返回值，那么返回值就会放置在异常的`value`属性中

##### 传参数

如果使用`a = yield b`这种形式，代表的是可以传参，或者说应该传参。

~~~python
def t() :
    num = 0
    while True :
        a = yield num
        print(a)
        num += 1
~~~

上面的生成器的特点是可以接受外面传入的参数，把参数赋值给a。

于是，到此时为止，驱动生成器向前运行至断点的方法不再是只有一种了，而是`next()`和`xxx.send(10)`这样两种方法。两种方式不同之处在于后者可以传参进去。但是两种方法还有一个区别就是，send不能用于启动生成器，也就是说生成器一开始就必须调用一次next，之后才能使用send或者next。

如果生成器有接收参数，但是却用了next，那么会自动传入None，如果生成器没有接受参数，却使用了send，那么生成器会正常工作，只是传入的东西没用而已。所以基本就是这样。



总而言之的就是，生成器的特别之处就是可以保存函数的执行状态断点，但是却完全像调用普通函数一般，不存在过多的切换，比线程切换更节省资源，缺点就是在于要手动切换。

其实对于GIL存在下的多线程来说，处理机还是只有一个，所以此时的多线程依旧是在保持高速的时间片切换。而生成器可以作为协程使用，它的特点是我们也可以完成单个处理机下的任务高速切换，只是此时的切换是我们手动指定顺序的。

~~~python
import time

def work1() :
    n = 20
    while n> 0 :
        print("this is work1")
        time.sleep(0.5)
        yield
        n -= 1
        
def work2() :
    n = 20
    while n> 0 :
        print("this is work2")
        time.sleep(0.5)
        yield
        n -= 1
    
def main() :
    a = work1()
    b = work2()
    try :
        while True :
            next(a)
            next(b)
    except :
        print("massion done....")
~~~

上面就是一个协程示例，利用yield作为断点，通过next轮流调用两个工作，交替执行。明显假设我们有两个任务，我们通过生成器改造这两个任务，为它们增加断点。然后通过类似main这样的循环调用，那么只要他们每个断点之间执行的时间足够短，或者说时间片足够小，那么，在用户看来，这两个任务不就是在同步执行吗？这就是利用协程实现多任务。



但是，明显似乎绝大多数情况下，这样并不是特别实用，因为我们很少关心表面上的多任务，回想一下，在GIL存在的情况下，为什么我们会使用多线程做爬虫，明明只有一个处理机？因为我们知道python的系统会自动监控那些不占用处理机的I/O阻塞操作，在某个线程执行这种操作的时候，就把它从处理机切出去，然后执行其他任务，这才是实现了加速的关键。



所以，如果不是为了实现表面上的多任务，而是真的为了加速而是用协程，那么关键也就是让协程实现在不占用处理机的阻塞操作处实现自动切换，明显即便知道了上面的协程方式，我们也做不到这个。所以，我们需要使用模块，例如greenlet和gevent

简而言之，greenlet基于生成器协程，它的作用是类似于一个框架，稍微简化我们的操作，因为明显上面的例子，为了实现协程的管理，我们自己构建了一个main函数，实现切换和管理，greenlet可以帮助我们简化，不需要自行管理了。。。。。但是明显，它似乎也没啥特别的，没有也行，只是复杂一点而已。

然后祭出gevent，这个库是一个网络并发库，基于greenlet，它重新构建了那些socket这种东西，把它改造成了可以在阻塞时完成切换的。

