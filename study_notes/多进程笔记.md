## 多进程笔记

我准备开始学习一下python的多进程模块，multiprocessing

目标：首先要学会最基本的使用，然后是协同工作与通信，再然后是进程池



### 基本版本

和多线程一样，最开始我们应该使用函数定义工作，然后创建实例，逐个开始，然后join，让主线程阻塞，直至各个进程完成工作。

~~~python
import multiprocessing as mp
import time

def work(name) :
	print("%s is begin in %s..."%(name,time.time()))
	time.sleep(4)
	print("%s is end in %s..."%(name,time.time()))


if __name__ == "__main__" :
	print("work begin in %s"%time.time())
	P = []
	for i in range(5) :
		p = mp.Process(target = work,args=(i,))
		P.append(p)
		p.start()
	for i in P :
		i.join()

	print("work end in %s"%time.time())

~~~

要注意，多进程要求运行的代码必须放在`if __name__ == "__main__" :`当中

我觉得有必要解释一下什么是join，在多线程中我说过如果A调用了B的join，那么A会一直等在这个位置等待B，直至B结束

对于多线程而言，主线程相当于一个等待和调度者，当主线程循环调用各个子线程的join的时候，实际上首先调用了1的join，然后主线程将阻塞，直至1完成，然后再调用2的join，但是因为之前各个线程已经start了，所以主线程的阻塞并不会影响子线程的执行，join只是让主线程等待各个线程的结束信号，如果调用2的join的时候,2早就结束了，那么主线程也会迅速闪过去，如果没结束，主线程会继续阻塞。

对于进程而言，也是如此，所以完全不需要认为join会让各子进/线程顺序进行，不会的，一旦调用完start，各个子进/线程就已经开始工作了。

可以去看一下多线程部分，我做的关于join的声明，它完全适用于进程，如果我们并不需要主进程在各子进程结束后做什么工作，那么完全没有必要调用join。

也就是说设置守护线程和join在某些角度上拥有完全一致的效果。

### 资源共享

第一反应肯定是用lock，多进程模块里面也提供了自己的锁，但是你应该知道对于多进程而言有些问题复杂得多。

首先，会想到搞一个列表，每个进程都互斥的去写这个列表，然后我们应该就可以拿到修改之后的了，这个推断建立在这样的基础上，对于python，构造型的类型传入函数的时候传递的是引用，无需返回就可以保留修改。但是对于进程而言，出现了一个极其严重的问题，每个进程都有自己的内存空间，所以他们并不与主进程共享内存，所以写列表的结果是无法同步出来的。

接下来会想到什么，文件。我们可以直接让他们写同一个文件，这样强制让他们访问相同的地址就好了吧。但是这时还是要注意一个十分重要的问题：我们不能在主进程打开文件，然后传入文件描述符，这样绝对会报错。我们只能传入文件名，由各个子进程自己使用锁互斥来打开，这样才能顺利工作。

~~~python
import multiprocessing as mp
import time

def work(name,k,res) :
	print("%s is begin in %s..."%(name,time.time()))
	time.sleep(4)
	k.acquire()
	with open(res,'a') as fi :
		fi.write(str(name)+'\n')
	#res.write(str(name)+"\n")
	k.release()
	print("%s is end in %s..."%(name,time.time()))


if __name__ == "__main__" :
	print("work begin in %s"%time.time())
	fi = "t.txt"
	lock = mp.Lock()
	P = []
	for i in range(5) :
		p = mp.Process(target = work,args=(i,lock,fi))
		P.append(p)
		p.start()
	
	for i in P :
		i.join()
		print("join one")
	#fi.close()
	print("work end in %s"%time.time())
	#print(res)
~~~

这样限制的确有些大，有没有其他方法？

先岔开一下，说一点和锁有关的

锁代表只有一个资源，也就是每次只能由一个子进程访问资源，大多时候大多数资源的确只能让一个进程访问，但是也有一些资源是允许有限个进程同时访问的，这是锁就不适合用来做控制了，此时，我们可以使用信号量：`Semaphore`

我并没有亲手写，附一个网上的代码

~~~python
import multiprocessing
import time

def worker(s, i):
    s.acquire()
    print(multiprocessing.current_process().name + "acquire");
    time.sleep(i)
    print(multiprocessing.current_process().name + "release\n");
    s.release()

if __name__ == "__main__":
    s = multiprocessing.Semaphore(2)
    for i in range(5):
        p = multiprocessing.Process(target = worker, args=(s, i*2))
        p.start()
~~~



进程之间的同步通信可以使用Event

Event在线程里面也有相同的意义，这里统一说明。首先需要实例化Event()类，实例有四个方法:

-   `clear()`,将flage设置为False
-   `set()`，设置为True
-   `is_set()`，检测一下flage是否为True
-   `wait()`，持续监听flage，阻塞中，直至flage为True



队列Queue

说回通信，在多进程中实现通信的方式大概有Queue，Pipe，或者是共享数据块，或者是Manager，其实方法还是很多样的。

Queue是多进程安全的，无论是多线程还是多进程中的Queue都来自于queue模块，当实例化Queue的时候，有一个缺省参数maxsize，缺省值是0，当maxsize<=0的时候，代表Queue的尺寸是无穷的。

我想起来了，在写线程池的时候，我应该有详细研究过Queue，但是我为什么没有写笔记，靠！

我们可以让子进程统一向Queue中写入数据，主进程负责读，就可以完成信息同步的作用。

~~~python
import multiprocessing as mp
import time
import numpy as np

def work(name,que) :
	print("%s is begin in %s..."%(name,time.time()))
	time.sleep(4)
	data = np.ones([26,1],dtype=np.float32).flatten()*name
	que.put(data)
	print("%s is end in %s..."%(name,time.time()))


if __name__ == "__main__" :
	print("work begin in %s"%time.time())
	res = np.array(range(26), dtype=np.float32)
	res = res*0
	#lock = mp.Lock()
	que = mp.Queue()
	P = []
	for i in range(5) :
		p = mp.Process(target = work,args=(i,que))
		P.append(p)
		p.start()
	
	for i in P :
		i.join()
		print("join one")
	#fi.close()
	for i in range(5) :
		res += que.get()
	print("work end in %s"%time.time())
	print(res)
~~~

其实这样已经可以了，足够完成任务了。



管道Pipe

手册上并没有明说管道是否是多进程安全的，我觉得应该是不安全的，但是测试的结果似乎表明它是安全的，但是我不敢确定，因为测试的时候子进程有一点时间差，所以可能是刚好错开了而已。

管道在创建的时候，可以传入duplex参数，这个参数默认是True，代表管道是全双工的，即两端都即可读又可以写。创建之后，返回值是一个二元元组，分别是管道的两端。每一端使用send()输入，使用recv()接收。

管道似乎并没有容量限制，手册说每次传送的数据如果超出阈值会引发错误，典型值是32M

~~~python
import multiprocessing as mp
import time
import numpy as np

def work(name,que) :
	print("%s is begin in %s..."%(name,time.time()))
	time.sleep(4)
	data = np.ones([26,1],dtype=np.float32).flatten()*name
	que.send(data)
	print("%s is end in %s..."%(name,time.time()))


if __name__ == "__main__" :
	print("work begin in %s"%time.time())
	res = np.array(range(26), dtype=np.float32)
	res = res*0
	#lock = mp.Lock()
	#que = mp.Queue()
	parent,son = mp.Pipe()
	P = []
	for i in range(5) :
		p = mp.Process(target = work,args=(i,son))
		P.append(p)
		p.start()
	
	for i in P :
		i.join()
		print("join one")
	#fi.close()
	for i in range(5) :
		res += parent.recv()
	print("work end in %s"%time.time())
	print(res)
~~~



另外一种低级一点的共享内存方式是Value或者Array。实例化Value或者Array，各进程共享这同一块内存。

Array来自多进程模块，创建时应该提供两个参数，数据类型和初始值，基本的数据类型是`'i'或'd'`，前者代表int后者代表double，其实还有一个缺省参数是lock，默认值是True，代表着自动进行了互斥操作，因此是进程安全的。

一个不太方便的地方是，传入的初始数据是np.array，但是你在写数据的时候依旧要循环逐个写，不支持直接相加

~~~python
import multiprocessing as mp
import time
import numpy as np

def work(name,que) :
	print("%s is begin in %s..."%(name,time.time()))
	time.sleep(4)
	data = np.ones([26,1],dtype=np.float32).flatten()*name
	for i in range(26) :
		que[i] += data[i]
	print("%s is end in %s..."%(name,time.time()))


if __name__ == "__main__" :
	print("work begin in %s"%time.time())
	res = np.array(range(26), dtype=np.float32)
	res = res*0
	#lock = mp.Lock()
	#que = mp.Queue()
	#parent,son = mp.Pipe()
	arr = mp.Array('d',res)
	P = []
	for i in range(5) :
		p = mp.Process(target = work,args=(i,arr))
		P.append(p)
		p.start()
	
	for i in P :
		i.join()
		print("join one")
	#fi.close()

	#for i in range(5) :
	#	res += parent.recv()
	print("work end in %s"%time.time())
	print(arr[:])


~~~

相比于这样低级的共享内存，Manager更高级一点，它支持更多的类型，例如字典，列表，锁，队列等等，我实际上是没有特别多的兴趣的，看一下multiprocessing模块手册开始的Server process部分，有例子。



然后呢，好像其实还有办法让工作函数具有返回值，好像用的是map，但是操作起来似乎会更加繁琐一点，所以我选择队列。



### 进程池

满足基本需求的最后一部分。

我没有心情做十分准确地探究，够用就好，所以现在的情形是，我可以做到线程池完美工作，但是绝对不能传入一个队列作为参数。

~~~python
import multiprocessing as mp
import time
import numpy as np

def work(name) :
	print("%s is begin in %s..."%(name,time.time()))
	time.sleep(4)
	data = np.ones([26,1],dtype=np.float32).flatten()*name
	
	print("%s is end in %s..."%(name,time.time()))
	return data


if __name__ == "__main__" :
	print("work begin in %s"%time.time())
	res = np.array(range(26), dtype=np.float32)
	res = res*0
	que = mp.Queue()
	pool = mp.Pool(3)
	p = []
	for i in range(10) :
		p.append(pool.apply_async(func=work,args=(i,)))
	
	
	pool.close()
	pool.join()
	print("all work join")
	
	for i in p :
		res += i.get(timeout=10)

	print("work end in %s"%time.time())
	print(res)
~~~

因为没有准确的研究，现在我能得出的半猜测性的结论是：

进程池内部就像我的线程池一样，肯定使用了队列，莫名其妙的原因，你传入一个队列进去，就会被干扰。所以绝对不能使用队列。

我们应当使用异步的方式将任务传入池中，然后关闭进程池传入任务的通道，然后阻塞进程池。

进程池会阻塞在添加任务的位置，直至所有的任务添加完成。

虽然我们无法使用queue获取结果，但是进程池为每一个任务都提供了一个get方法，这个方法会返回任务的返回值。

所以同样可以工作。

就这样吧，先。



说实话，我依旧不知道内部的工作方式，GIL导致多线程无法有效利用多核CPU的优势，网上说多进程利用的就是d多核，现在的电脑是4核的，所以理论上来说，开超过4个进程大概是没效果的吧，但是事实上，我测试了一下，10个任务，从3进程，到4进程，5进程，10进程，速度呈规律提升，所以没道理呀，哪有10个处理机给它用，并且系统还有其他的工作要进行。

这里唯一的漏洞在于我测试时使用了sleep进行延时。接下来我会进行一次计算密集型测试。



### 一次测试记录



| 任务数  | 进程数  | 用时      | 与单进程的速度比 | CPU利用率             |
| ---- | ---- | ------- | -------- | ------------------ |
| 10   | 1    | 1968s   | 1        | 约20%               |
| 10   | 4    | 545.5s  | 3.611    | 前一部分100%，最后两个任务50% |
| 10   | 5    | 454.4s  | 4.334    | 100%               |
| 10   | 10   | 459.18s | 4.287    | 100%               |



所以，计算密集型，的确是4核就差不多了，但是还要跟任务数相匹配，所以，基本上5进程就到了极限，再增加基本用时不变，甚至略微下降。



与此做一对比，matlab完成上述计算大概用时会在177秒左右，速度大概是python的2.5倍左右，但是这并不是严格的对比，首先matlab并没有使用全部的cpu，它对CPU的利用率只有30%，并且177秒也只是个例，有时会更快，也许会更慢。而对于python而言，不公平的地方在于我进行了大量的IO操作，我读写了很多的文件，大概每个任务会写26份文件，读26份文件，每份文件1270KB，所以，结论就是python的确比matlab慢，但经过优化，可以让速度比大概保持在10倍以内，也许更小，甚至是5倍，3倍。

