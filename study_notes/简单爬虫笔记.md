# 简单爬虫笔记

一直以来，从来没有做过知识梳理，这里做个简单的记录



### 工具与环境

这里只是简单的爬虫，针对至多一个站点的量级，针对如何选择爬虫，可以参见[这里](https://www.zhihu.com/question/26790346/answer/34193485)写了pyspider的我邮大神学长[足兆叉虫](https://www.zhihu.com/people/binux)的回答，话说他的网名实际上是跳蚤的意思？

我们使用python，主要考虑里面的urllib，request，bs4,re这些模块

同时作为辅助，还会使用我写的线程池

然后，对于普通的页面，不需要额外的工具，基本上，(浏览器肯定是要的，建议chrome)，但是对于需要登陆的页面，问题就会变得复杂得多，我们必须使用抓包的工具检测我们向哪个页面发送了什么表单，此时需要[Fildder](https://www.telerik.com/fiddler)，自行下载，傻瓜式安装，然后针对https的链接，我们为了获取信息必须让浏览器信任fildder，所以需要安装证书， 最好下载[证书生成器](http://fiddler2.com/r/?fiddlercertmaker)，如果这个链接不行的话，直接谷歌fildder cert maker，自己找。下载完安装即可。

打开fildder，找到Tools > Fiddler Options > HTTPS 选项卡，勾选Decrpt HTTPS traffic，然后一路确定，最后勾选Check for certificate revocation，也许可以不选。然后点击Actions，选择导出证书到桌面，然后按照默认选项，添加到chrome里面即可。

你可以打开一个需要登陆的页面，然后在fildder里面点击叉号，清空以前的追踪记录，然后在浏览器里面输入登录信息，点击登陆，回到fildder里面自己找吧，找那些带个绿色向右箭头的记录，他们代表发送的信息，一般而言，登陆的页面的URL都会带有login的字符，找到这些记录，点击界面右侧的WebForms即可找到发送的表单。



### 开始

最简单的情况下，我们使用的是urllib.request模块

~~~python
from urllib import request

a = request.urlopen('https://www.douban.com/')

html = a.read()
~~~

上面的a的类型是`http.client.HTTPResponse`，我们要获取页面，必须通过他的read方法，返回值是二进制的html页面。

在urllib里面提供了很多其他有用的模块，但是事实上，这个模块我真的已经很久都没有用过了。

我最常使用的是requests模块。

考虑一下，我们要做的任务是什么？绝大多数情况下，我们只需要GET即可，拿到页面之后，爬虫的工作其实已经结束了，接下来接手的是页面分析，也许我们直接就可以从页面提取到感兴趣的信息，也许我们还要从页面中拿到我们感兴趣的链接，再做下一步的请求。

但是这只是针对最简单的情况，可以说网站完全没有做任何的防护工作，稍微聪明一点的网站都会去检查你的请求报文的Headers，如果发现你不是浏览器，而是脚本，就会忽略你的访问。

所以说，最基本的，我们可能会需要携带一个Headers去访问，然后在这个里面伪装自己。

真正麻烦的问题是登陆，一旦需要登陆，我们就必须向服务器发送POST报文，表单什么的，然后服务器可能会通过cookie什么的帮你保持登录状态，这些使用urllib进行处理，大概都会比较麻烦，所以深入的urllib的知识不在讲述，感兴趣的话可以去看文档，也许你会说，我想要最基础的知识，那么呵呵了，urllib才不是最基础的，socket才是相对靠近底层的，你可以去试试使用socket完成这一切。但是这样做完全就是在重新发明轮子，无意义，并且效果绝对不会好。当然，你真的想学习基础的情况下除外。使用python的原因就是为了快速上手。



#### requests

对于requests的应用也可以分为低级和高级，低级的应用直接使用模块里面的get，post等方法，最基本的还是get方法

`r = requests.get('https://www.douban.com')`

返回的是一个Response对象，这个对象的属性和方法可以使用dir获取。我们常用的是`status-code`属性，这个属性给出了请求的状态

然后content属性包含的是相应内容，但是是二进制格式的

encoding属性是发现的编码格式，但是一般而言这是不准确的，`apparent_encoding`是从内容猜测出来的编码格式，我们应该这样做`r.encoding = r.apparent_encoding`

text属性给出了使用encoding解码的二进制内容转换出来的字符串



当我们想携带自定义的headers的时候，只需要将headers写成字典的格式，然后在调用get等方法的时候，把它传递给headers这个参数即可。

根据当时的网络编程经验，我们可以知道，对于get请求而言，最简形式的headers只需要给出`User-Agent`即可，我们可以使用`Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36`伪装为浏览器

对于post请求而言，理论上来说，我们还应该指定`Content-Type`和`Content-Length`，但是事实上，我几乎从来没有写过，也是可以工作的

对于post而言，到目前为止，我用过的只有提交登录信息的表单，我们只需要把表单写成字典格式，然后发送即可

例如：

~~~python
postd = {"_xsrf":xsrf,
         "phone_num":"xxxxxxx",
         "password":"xxxxxxx",
         "captcha":captcha}

header1 = {
        'Host':'www.zhihu.com',
        'Connection':'keep-alive',
        'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36',
        'Accept':'image/webp,image/*,*/*;q=0.8',
        'Referer':'https://www.zhihu.com/',
        'Accept-Encoding':'gzip, deflate, sdch, br',
        'Accept-Language':'zh-CN,zh;q=0.8',
        'X-Requested-With':'XMLHttpRequest'
}

url = 'https://www.zhihu.com/login/phone_num'
s = session.post(url,postd,headers=header)
~~~

对于图片内容，我们只需要拿到每一张图片的地址，请求这张图片，然后以二进制格式写入图片中即可，对于验证码，我们只需要`pic = PIL.Image.open(io.BytesIO(pic.content))`，然后显示即可



这是对于常规的请求，当网站进行了登陆之后，我们需要完成保持登陆状态的操作，一般而言，这是通过cookie实现的，但是，手动的处理会比较麻烦，所以我们推荐使用requests的session

只需要实例化一个`requests.Session`类，然后调用实例的get和post等方法即可，会话类会自动完成所有的保持状态的操作。

基本上，绝大多数情况下，到这里已经够用了，更深入地了解，包括更高级的操作，交互，直接去看requests的文档即可，内容很简单，也非常简短，甚至还有中文版。



### 解析

当完成页面的获取之后，我们做的操作就是对页面内容的解析，这里主要应用bs4和re模块

前者用于结构化html页面，后者用于正则表达式。

`bs2 = BeautifulSoup(s.text,'html.parser')`，很简单，只需要这样，将页面的字符串传入`bs4.BeautifulSoup`即可完成解析和结构化，一般我们应该指定解析器，`'html.parser'`差不多就可以，但有时可能会警告，让你改成html5什么的，看着该改的话改改就好，不该估计也不至于异常。

哦，解析器原来也可以安装，例如`pip install html5lib`和`pip install lxml`这样的

废话不说，那么解析出来的结构怎么使用呢？

我们考虑一份html文档，文档的主要内容基本上就是以标签来组织的，标签自身包含了一些属性，可能有子标签，也可能会包含一些文本内容。

所以单纯对于html来说，结构是十分清晰地，而bs4也非常合理化的将文档的内容包装为了四种类型，`Tag` , `NavigableString` , `BeautifulSoup` , `Comment` 

第一个就是标签，第二个指的就是标签的文本内容，第三个指的是文档自身，也就是说上述的bs2变量就是这种类型的对象，第四个指的是文档中的注释内容。

文档的每一个标签都是`BeautifulSoup`对象的一个属性，例如标签a，就是`bs2.a`

当同一类标签有很多的时候，只会返回第一个。

对于每一个标签，类型都是Tag，每一个Tag都有诸如name这样的属性。

那么html标签的属性怎么拿到呢？我们既可使用`.`索引，也可以使用[]，对于某些属性，html5规定这些属性可以是多值属性，那么bs4会以列表的形式返回多个值，如果没有规定这是一个多值的属性，却写了多值，那返回的只是一个字符串而已

标签包含的字符串文本会被存储在Tag的string属性中，这是一个`NavigableString`类型



#### 树形结构

标签也会包含子标签，也可以使用`.`访问，例如：`bs2.body.div`，但是还是那句话，必须注意`.`索引只会返回第一个

我们注意，在bs4里面，将常使用节点的概念，所谓节点实际上包含了上述四类对象，也就是说标签，标签的字符串内容都是节点。

如果我们想获取一个标签的子节点，我们可以使用`contents和children属性`，这两个属性在表现上基本上是一致的，但是前者一个列表，后者却是一个迭代器，所以常用后者执行迭代任务

但是要注意的是，这个列表里面只包含直接子节点，换句话说，例如标签div包含了a，a又包含了img，那么列表里面并不会包含一个img而是只有一个包含了img的a

是否有办法递归解析出子标签的子标签这样的结构呢？使用`descendants`属性，这个属性是一个生成器，递归解析了所有的子孙标签，但是我们只能使用for循环遍历



当一个标签只有文本内容的时候，我们可以使用string属性拿到文本，但是包含了文本还包含了子标签，子标签还有文本的时候，就不行了，这时候应该使用strings，这个属性也是一个生成器，然后要想去除多余的空白行的话，应该使用stripped_strings属性

##### 父节点

节点的父节点是parent属性，html文档的父节点是BeautifulSoup对象，BeautifulSoup对象的父节点是None

要想递归得到所有的父节点应该使用parents，同样的这是一个生成器，会递归到None为止

##### 兄弟节点

`.next_sibling`和`.previous_sibling`

还有`.next_siblings`和`.previous_siblings`

要注意如果两个相邻标签之间有换行什么的，你拿到的下个标签实际上是换行符，小心一下



还有其他的elements的前进后退的手段，但是真的事实上我很少用



#### 文档搜索

我真正常用的方法还是搜索文档，在这里最常用的又是`find_all`，你可能会注意到`findAll`这个方法，后者已经被废弃了，取而代之的正是前者。

深入讲的话，这里也能说很多，但是，我将只说最普通的使用方法，传入一个参数是标签名，如果还想指定标签的属性，应该使用字典的形式传入属性值，为了进一步的过滤，属性的值可以使正则字符串，只需要传入`e.compile("sisters")`即可。

算了，就这样吧，先。







## Scrapy

现在我决定开始使用scrapy。主要参考资料室官方文档，虽然是英文版的，但是并不多，不到300页而已。这里不会做太多过于详细的记录。



### 模式

一般的模式是：

1. 通过命令行开始一个scrapy项目
2. 为这个项目增加一个爬虫文件，其实主要是解析器
3. 通过命令行运行

大概就是这些。

开始项目`scrapy startproject learn`。会自动新建一个learn文件夹，下面就是这个项目的所有文件和子项目。

在`learn/learn/spiders`目录下新建一个代码文件，名字自拟，例如`spider.py`，代码如下：

~~~python
import scrapy

class BookSpider(scrapy.Spider) :

    name = "books"

    def start_requests(self) :
        urls = ["http://quotes.toscrape.com/tag/humor",]

        for url in urls :
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response) :
        for quote in response.css("div.quote") :
            yield {
                "text": quote.css("span.text::text").extract_first(),
                "author": quote.xpath("span/small/text()").extract_first()
            }

        next_page = response.css("li.next a::attr('href')").extract_first()

        if next_page :
            yield response.follow(next_page, self.parse)
~~~

至于结构和功能很明显了，程序会自动调用start_requests方法，生成请求，结果会自动调用parse解析。

需要解释的就是scrapy是异步的，会自动发起很多请求，然后得到返回结果的时候会自动调用解析。



启动方法：

回到顶层目录，即learn目录下，执行命令`scrapy crawl books`就可以开始执行了，当然由于没有保存，将得不到什么数据文件。命令的最后一个参数就是爬虫的名字。



唉，算了，真的说起来就没完了，看文档吧，很简单的。下面只记录一些点。



`start_requests`方法和`start_urls`属性一致，后者更简单，所以必须实现默认的解析方法`parse`



scrapy很贴心地提供了一个shell可以作为数据解析的练习环境。执行`scrapy shell "http://quotes.toscrape.com/page/1/"`命令，它会自动帮你下载这个网页，以一个response的形式提供，接下来就可以练习解析了。输出的东西里面可以看到在这个shell里面可用的对象都包含了什么

