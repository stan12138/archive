# pandas学习笔记

>--by stan 
>2017.6.19

   


有两种数据结构,`Series`和`DataFrame`，前者是一维的数据结构，后者是2至更高维度的。

### Series

#### 创建

Series可以接受列表、字典或array等一维结构的数据，支持传入一个index变量作为索引

为了方便说明，可以针对输出时的表现说，Series是一种列向量，序号叫做index，自然情况下，index是自动生成的`0~n`的数字，我们可以通过传入index参数指定，或者字典中自带

#### 数据使用

使用index索引，计算过程中会根据索引自动对齐

对于series来说，索引很重要，如果创建时不传入index参数的话，将会自动创建数字索引，计算过程中元素也是以索引为标准对齐的

然后要注意的是，索引支持和Numpy一样的方式，如冒号切片，列表索引多个项等等，另外要注意的是数字序号的索引是自带的，即使指定了其他索引，也可以继续使用数字索引。但是要切记的是，一般情况下，我们如果要指定索引的话是不会制定数字的，例如可以将索引指定为`1~n`，这种情况下，即便你这样做了，在冒号索引的时候，系统也会认为你用的是`0~n`的天然索引，其余情况下，例如单个索引，或者列表索引则会认为是自行制定的索引，总之，会造成很大的混乱，不要这样做。

当使用`obj["a":"b"]`这种索引的时候，特别，尾部是包含的





### 数据加载

接下来在DataFrame中，数据量都比较大，使用手工创建字典的方式会比较累，所以，这里先提前记录一下关于从文件中加载数据的方法。

负责加载数据的函数主要有四个：

-   read_csv
-   read_table
-   read_fwf
-   read_clipboard

最常使用的是前两个，现阶段，我使用的可能是read_csv

也可以使用read_table加载csv文件，但是要通过sep参数指定分隔符

直接使用`read_csv('t1.csv')读取，数据的第一行会被作为列标，行标则会自动生成，从0开始。可以使用names指定列标，行标只能先选择使用默认数字，或者某一列

`data = read_csv('t1.csv',names=['a','b','c','d','name'],index_col='name')`





### DataFrame

#### 创建

创建的时候，我认为常用的方法是，1、传入二维数组或者列表，2、传入字典，3、读取csv文件

要知道的是，DataFrame有两个索引，行标称为index，列标称为name或者columns，正如惯例，每一行代表一条记录，每一列代表一个类别

当传入二维数组的时候，index和columns都是自动生成的数字

当传入由列表做成的字典的时候，键会被当作columns，index则会自动生成，当然这些都可以通过index，columns参数指定

当使用`read_csv`读取的时候，因为按照csv文件的规范要么第一行空出来，要么第一行就是列名，所以在这里会自动取第一行为表头，而index则会自动生成

#### 使用

索引的时候，使用`obj["state"]或obj.state`这样的形式可以对columns进行索引

`obj.index, obj.columns, obj.values`可以分别得到行标，列标，二维数组值，其中前二者是Index对象

我们可以直接通过对`obj.index,index.columns`赋值，来实现重新索引，这只是重新命名，有的时候想完成的另一种操作叫做reindex

drop方法可以丢弃指定的轴上面的一到多项，使用axis指定哪条轴

索引比较奇怪，一般情况下的确是完成的列索引，但是如果采用了切片，或者布尔数组可以完成对行的索引

但是不能使用单个或列表式的多个完成对行的索引

特别的，如果你想实现类似于numpy那样的对行的索引，或者同时索引行和列，那么应该使用ix字段，如`obj.ix[1,"name"]`

总而言之，索引的方法很多，直接式，ix，icol，irow，`get_value`等

ix索引被舍弃了，官方决得它会导致大量的误解，造成困惑，因而，被舍弃了。



### 操作与运算

当两个DataFrame进行算术运算的时候，两者之间不重叠的全部会被置为NaN，如果使用算数方法的话，可以设置为特定值

当DataFrame和Series之间进行算术运算的时候，所遵循的规则是广播和索引对齐



以numpy为例，如果用数组减去他的一行，那么结果是每一行都减去这一行，DataFrame与此相同，但是这个也是按照索引来执行的，如果索引不同还是不匹配位置为NaN

当然我们可以提取一列，然后如果想要每一列都减去这一列的话，必须使用算术方法，并指定匹配的轴，对于这个操作，axis=0



此外可以对于DataFrame使用numpy的函数

此外可以使用apply方法应用一个函数，可以通过aixs指定应用于行或者列，然后还可以使用applymap对每个元素执行



#### 排序

根据字母表顺序对于index进行排序，可以使用`sort_index方法`，也可以通过axis对columns排序，可以指定升降序

如果要按值排序的话，对于Series直接使用order方法，对于DataFrame应该使用`sort_values(by=,axis=)`来完成依照任意行或者列的排序

要注意的是，可以同时依照多个columns进行排序，如`b.sort_values(by=[4, 0])`，这种情况下实际上是先依据0排一次序，然后根据4排序



还有排名是什么鬼



事实上，无论是Series还是DataFrame都没有限制index或者columns的名字不能重复



### 统计

`obj.sum(axis)`可以按行或者列求和，这里NaN会自动被排除

`obj.mean(axis)`求均值

`obj.idxmax(axis)`最值的index

`obj.cumsum()`

`obj.describe()`

相关系数与协方差



`Series.unique()`获取唯一值数组，`Series.value_counts()`计算各个值出现的频数，另外也可以使用`pd.value_counts()`来查看任意数组和序列

isin方法可以判断Series的各个值是否包含在传入的值序列中

可以把这些针对Series的函数传递给DataFrame的apply方法



#### 缺失数据处理

NaN可以通过np.nan来制造，检测方法包括，fillna,isnull,notnull

对于Series，直接利用dropna方法即可把缺失的数据清理掉，对于DataFrame如果直接调用这个方法默认是把所有包含确实数据的记录清理，即去掉这些行。可以通过axis参数指定列，可以通过`how="all"`来设置之丢失那些全部都是缺失的，thresh参数指定如果该行或者该列超过几个缺失数据就被丢弃

`dropna(axis, how, thresh)`

填充的时候主要使用fillna(value,method,axis,inplace,limit)

value可以指定用什么值填充，写成一个字典的话可以针对不同的列或者行填充不同值

method是填充方法，inplace决定是返回新对象还是就地修改，limit也是决定填充方法的

#### 层次化索引

无论Series还是DataFrame它们的每个轴实际上都可以有多重索引，这个其实我并没有遇到实例，暂时跳过

#### 列与索引

有的时候，原始数据中实际上把索引作为一列植入了DataFrame中，也许是层次索引，那么就是两列，此时，我们想要设置索引，那么应该使用`set_inde(value, drop)`，value是集合的时候就是层次索引，drop决定是否继续保留这些列

`reset_index`是相反的操作，会把索引放回去



关于之前提到的整数索引的问题，实际上都是从避免误解的角度考虑的



### panel数据

panel数据是高维的数据结构



### 文件读写操作

除了前面提到的基本操作之外，还有一些高级操作，暂时跳过



### 数据规整化



#### 合并操作

合并一般使用`pd.merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy)`

合并的时候，最一般的情形下，系统会自动寻找到两个dataFrame的某一列列名一致，然后根据这一列进行合并，如果有多个列一致的时候，我们可以通过`left_on和right_on`指定左右各使用哪一列，对于某些数据，我们实际上想使用index作为参考进行合并，此时，只需要通过`left_index或right_index`来设置

可是合并其实更恶心，让人搞不懂，最简单的情形下，我们有第一个数据表，也许是index也许是某一列代表每个用户的id，然后其余的列可能是name,years old等数据，然后第二个表里，存在着一列id，然后其余列是class time等数据，此时的合并目标很清晰，就是为id增加一些新的数据列，合并的结果也正如我们所想。当时你可能会注意到，结果的行数有点不太对，这不是什么大问题，how参数有四个可选值，inner,outer,left,right，他们只决定会有哪些行出现在结果中，默认是inner，即交集，也就是说只有同时出现在两个表中的id才会出现在结果中，其余的被删除，outer是并集，这样的话，只要一个表中有某个id那么结果中就会存在，缺失的列将会被标记为NaN，至于left，right也很明显了

但是问题恶心的地方出现在作为合并指导的这一列存在多个重复的项，例如id这一列有多个行具有相同id，例如表1有3个a，2个b，表2有2个a，3个b，那么结果会是怎样的？结果是笛卡尔积，即将出现6个a,6个b

很诡异，虽然在某些情况下也是可以理解的

然后，接下来实际上还有更为复杂的合并，例如赋值给参数on一个列表，可以让两个DataFrame依照多个键进行合并

如果依据索引进行合并，然后恰好索引是层次化索引，情况也会变得更加复杂



#### 连接

DataFrame的join方法实现的是一种连接，但是，却和按照索引进行的合并很类似，同时它还支持同时传入包含多个表的列表进行多表连接

首先要用的函数来自于pd，名字叫concat，这个方法用于Series或者DataFrame，当用于两个Series时，我们可以选择沿那个轴合并，默认工作在axis为0上面，结果是索引扩增，结果依旧是Series，此时存在的一个问题是我们无法分解出结果中哪个片段，方法是我们可以传入一个keys参数，制定一个名字列表，结果将是同样的Series，但是包含了层次化索引。另一种方式，我们可以调整参数axis，设置为1，结果将是一个DataFrame，行索引是所以的index，每一列都是原本的一个Series。这种情况下，我们同样可以指定keys，只是此时keys将变成结果的列头

如果将concat应用于DataFrame，在不做任何参数调整的情况下，结果将是行的并集，列的并集，重复的将会同时保留，此时如果指定了axis为1的话，相当于将每个表的列拆解出来，然后拼在一起，此时为了分辨，也可以指定keys，从而产生一个层次化索引



`combine_first`填补空缺数据？



#### 轴向旋转与重塑



